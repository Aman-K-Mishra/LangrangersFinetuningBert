{\"cells\": [\n  {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"# Fine-Tuning BERT for Text Classification\"]},\n  {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"## Introduction\"]},\n  {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"In this notebook, we will fine-tune a BERT model for text classification tasks. We will cover the following steps:\", \"1. Importing Libraries\", \"2. Loading Data\", \"3. Preprocessing\", \"4. Training the BERT Model\", \"5. Evaluating the Model\", \"6. Saving the Model\"]},\n  {\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"# Importing Necessary Libraries\",\n\"import pandas as pd\",\n\"import numpy as np\",\n\"import torch\",\n\"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\"]},\n  {\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"# Loading the Dataset\",\n\"df = pd.read_csv('path_to_your_dataset.csv')\",\n\"df.head()\"]},\n  {\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"# Preprocessing Data\",\n\"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\",\n\"train_encodings = tokenizer(df['text'].tolist(), truncation=True, padding=True)\"]},\n  {\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"# Loading the BERT Model\",\n\"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\"]},\n  {\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"# Training the Model\",\n\"training_args = TrainingArguments(\",\n\"    'my_model',\",\n\"    num_train_epochs=3,\",\n\"    per_device_train_batch_size=8,\",\n\"    per_device_eval_batch_size=8,\",\n\"    weight_decay=0.01,\",\n\"    logging_dir='./logs',\",\n\")\",\n\"trainer = Trainer(\",\n\"    model=model,\",\n\"    args=training_args,\",\n\"    train_dataset=train_encodings,\",\n\"    eval_dataset=eval_encodings\",\n\")\",\n\"trainer.train()\"]},\n  {\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"# Evaluating the Model\",\n\"results = trainer.evaluate()\",\n\"print(results)\"]},\n  {\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"# Saving the Model\",\n\"model.save_pretrained('path_to_save_model')\"]},\n  {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"## Conclusion\"]},\n  {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [\"In this notebook, we have successfully fine-tuned a BERT model for text classification.\"]}\n],\"metadata\": {\"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"}, \"language_info\": {\"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"file_extension\": \".py\", \"mimetype\": \"text/x-python\", \"name\": \"python\", \"nbconvert_exporter\": \"python\", \"pygments_lexer\": \"ipython3\", \"version\": \"3.8.5\"}}}